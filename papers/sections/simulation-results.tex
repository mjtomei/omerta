% Section 7: Simulation Results

\section{Simulation Results}
\label{sec:simulation-results}

The defenses described above are architectural arguments---they explain why attacks should be difficult, but not whether they actually are. This section presents simulation results that test the system's behavior under adversarial conditions, examine the economics of provider competition, and explore the machine intelligence demand thesis that motivates Omerta's design.

\subsection{Attack Scenario Outcomes}
\label{subsec:attack-outcomes}

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Scenario} & \textbf{Final Gini} & \textbf{Cluster Prevalence} & \textbf{Policy Response} \\
\midrule
Baseline (Honest) & 0.783 & 0.000 & Stable \\
Trust Inflation & 0.706 & 0.250 & $K_{\text{TRANSFER}}$ increased \\
Sybil Explosion & 0.641 & 0.545 & ISOLATION\_THRESHOLD decreased \\
Gini Manipulation & 0.882 & 0.000 & $K_{\text{PAYMENT}}$ decreased \\
\bottomrule
\end{tabular}
\caption{Attack scenario outcomes}
\label{tab:attack-outcomes}
\end{table}

\subsection{Key Finding: Structural vs.\ Parametric Attacks}
\label{subsec:structural-parametric}

The simulations revealed a striking pattern: automated policy adjustments trigger correctly but have limited impact on final outcomes. This suggests that attack effects are primarily structural rather than parameter-dependent.

\textbf{Implication}: Effective attack resistance requires architectural defenses that make attacks structurally infeasible, complemented by policy adjustments for fine-tuning. Parameter tweaking alone is insufficient against determined adversaries.

\subsection{Long-Term Stability}
\label{subsec:long-term-stability}

Five-year simulations showed stable trust accumulation under honest conditions and recovery between attack waves under adversarial conditions. The core trust mechanisms prove robust to extended adversarial pressure.

\subsection{Reliability Market Economics}
\label{subsec:reliability-economics}

A critical question for any decentralized compute network: can unreliable home providers coexist with reliable datacenters, or will one displace the other? We simulated provider competition under varying market conditions.

\subsubsection{Provider Cost Structures}

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Provider Type} & \textbf{Cost/hr} & \textbf{Reliability} & \textbf{Cancellation} \\
\midrule
Datacenter & \$0.50 & 99.8\% & Never (SLA) \\
Home Provider & \$0.08 & 92\% & For profit (1.5$\times$ threshold) \\
\bottomrule
\end{tabular}
\caption{Provider cost structures}
\label{tab:provider-costs}
\end{table}

Home providers have 6$\times$ lower costs because they already own hardware (no capex amortization), pay only marginal electricity, and have no facility overhead.

\subsubsection{Market Outcomes by Supply/Demand}

\begin{table}[H]
\centering
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Market Condition} & \textbf{Consumer Cost $\Delta$} & \textbf{DC Profit $\Delta$} & \textbf{Compute $\Delta$} \\
\midrule
Undersupplied (uniform values) & $-10\%$ & $\mathbf{-3\%}$ & $\mathbf{+200\%}$ \\
Undersupplied (mixed values) & $-52\%$ & $-66\%$ & $+200\%$ \\
Balanced & $-91\%$ & $-100\%$ & $+70\%$ \\
Oversupplied & $-84\%$ & $-100\%$ & $-8\%$ \\
\bottomrule
\end{tabular}
\caption{Market outcomes by supply/demand conditions}
\label{tab:market-outcomes}
\end{table}

\subsubsection{Key Findings}

\begin{enumerate}
    \item \textbf{Undersupplied markets show value creation}: When demand exceeds datacenter capacity, home providers serve unmet demand. Total compute delivered increases 200\% while datacenter profits remain largely intact (only $-3\%$ with uniform consumer values).

    \item \textbf{Balanced/oversupplied markets show displacement}: When total capacity meets or exceeds demand, home providers' 6$\times$ cost advantage allows them to undercut datacenters completely.

    \item \textbf{Consumer heterogeneity matters}: When consumers have widely varying values per compute hour, middle-market competition intensifies. Datacenters retain only the premium segment.

    \item \textbf{Reliability tradeoff is real}: In oversupplied markets, total useful compute can decrease ($-8\%$) because home provider unreliability causes more restarts despite lower prices.
\end{enumerate}

\textbf{Implication for Omerta}: The system creates genuine economic value only when \textbf{demand exceeds datacenter capacity}. This raises the question: will demand ever sustainably exceed supply?

\subsection{The Machine Intelligence Demand Thesis}
\label{subsec:mi-demand-thesis}

We argue that machine intelligence fundamentally transforms compute markets into \textbf{perpetually undersupplied markets}, making unreliable compute economically valuable for the foreseeable future.

\subsubsection{The Traditional View}

Human-driven compute demand is bounded. Businesses have finite workloads, consumers have finite entertainment needs. Markets tend toward equilibrium where supply meets demand. In equilibrium, price competition drives out high-cost providers.

Under this view, home providers would be viable only during transient demand spikes.

\subsubsection{The New Reality}

Machine intelligence creates unbounded demand because \textbf{machines can always find productive uses for additional compute}. Unlike humans, who run out of tasks to do, machine intelligences have continuous demand curves:

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Task Priority} & \textbf{Human Value} & \textbf{Machine Value} & \textbf{Notes} \\
\midrule
Frontier research & \$100/hr & \$0/hr & Requires human insight (for now) \\
Active inference & \$50/hr & \$40/hr & Real-time decision making \\
Background reasoning & \$20/hr & \$15/hr & Exploring solution spaces \\
Speculative search & \$5/hr & \$3/hr & Low-priority but positive value \\
Precomputation & \$1/hr & \$0.50/hr & Preparing for future queries \\
\bottomrule
\end{tabular}
\caption{Task value by priority level}
\label{tab:task-values}
\end{table}

The key insight: \textbf{there is always a next-best task}. A machine that cannot profitably do a \$50/hr task can still generate value doing a \$5/hr task. This creates a demand curve extending to arbitrarily low prices. \emph{Humans run out of ideas; machines run out of compute.}

\subsubsection{Implications}

\begin{enumerate}
    \item \textbf{No ``oversupplied'' market exists}: Machine demand expands to absorb any available compute
    \item \textbf{All providers can coexist}: Datacenters serve high-priority tasks, home providers serve the elastic tail
    \item \textbf{Price floors are set by marginal value, not marginal cost}: Machines bid what tasks are worth
\end{enumerate}

\subsubsection{Future Projection}

As machine intelligence improves, the value per compute hour increases at all quality levels:

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Era} & \textbf{Demand Characteristic} & \textbf{Market Structure} \\
\midrule
Human-only & Bounded, tends to equilibrium & Oversupply risk \\
Human + current MI (today) & Large, finite & Undersupplied \\
Human + superhuman MI (future) & \textbf{Unbounded} & Permanently undersupplied \\
\bottomrule
\end{tabular}
\caption{Demand evolution by era}
\label{tab:demand-evolution}
\end{table}

At the limit, superintelligent systems have unlimited demand for compute of any quality. Any machine cycle has positive value because it can be applied to self-improvement, exploration, or capability expansion.

\subsubsection{Conclusion}

The economic viability of Omerta depends on perpetual undersupply. Human demand alone cannot guarantee this. But machine intelligence---which can always find productive uses for marginal compute---transforms the market structure fundamentally. In a world of machine intelligences, the question is not whether unreliable compute will displace datacenters, but whether we can deploy enough compute of any quality to satisfy exponentially growing machine demand.

\subsection{Double-Spend Resolution}
\label{subsec:double-spend-resolution}

Unlike blockchain where double-spending is mathematically prevented by consensus, Omerta's trust-based currency can only detect and penalize double-spends after the fact. This raises questions about economic stability. We simulated five scenarios to validate the design.

\subsubsection{Detection Rate}

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Connectivity} & \textbf{Detection Rate} & \textbf{Avg Time} & \textbf{Network Spread} \\
\midrule
0.1 & 100\% & 0.046s & 97.6\% \\
0.5 & 100\% & 0.042s & 98.0\% \\
1.0 & 100\% & 0.042s & 98.0\% \\
\bottomrule
\end{tabular}
\caption{Double-spend detection by network connectivity}
\label{tab:detection-rate}
\end{table}

\emph{Finding}: In gossip networks, double-spends are always eventually detected because conflicting transactions propagate to common nodes. Connectivity affects detection speed, not completeness.

\textbf{Important clarification}: Detection is not prevention. A 100\% detection rate means the double-spend is always discovered---but only \emph{after} the conflicting transactions have propagated. During the detection window, both recipients may believe they have valid payments. The economic defense (trust penalties, Section~\ref{subsubsec:economic-stability}) makes attacks unprofitable but does not prevent the temporary confusion. For high-value transactions where even temporary double-spend would be harmful, use the ``wait for agreement'' protocol (Section~\ref{subsubsec:finality-latency}) which provides prevention through delayed finality.

\subsubsection{Double-Spend Detection Timeline}

The following diagram shows the sequence of events in a double-spend attempt:

\begin{lstlisting}[basicstyle=\ttfamily\small]
Time     Attacker         Victim A         Victim B         Network
------------------------------------------------------------------------
t=0      Sends TX1 -----> Receives TX1
         to Victim A      Believes valid

t=0.01   Sends TX2 ----------------------> Receives TX2
         to Victim B                       Believes valid
         (same coins)

t=0.02                    Gossips TX1 -------------------> Propagating
                                           Gossips TX2 --> Propagating

t=0.04                                                     Nodes see
                                                           BOTH TX1
                                                           and TX2

t=0.05                    <--- CONFLICT DETECTED --->      Broadcasts
                          Notified         Notified        conflict

t=0.06                    Keeps coins      Keeps coins     Records
                          (no clawback)    (no clawback)   double-spend

t=0.07   <-------------------------------------------------PENALTY:
         Trust destroyed                                   5x coins
         Future earnings = 0                               burned from
         Isolated from network                             attacker
\end{lstlisting}

\textbf{Key points}:
\begin{itemize}
    \item Detection window ($\sim$50ms): Both victims briefly believe they have valid payments
    \item Resolution: Both victims keep coins (inflationary, but victims aren't punished)
    \item Penalty: Attacker loses 5$\times$ the double-spent amount plus all trust
    \item Prevention alternative: ``Wait for agreement'' protocol delays finality but prevents the confusion window
\end{itemize}

\subsubsection{Economic Stability}
\label{subsubsec:economic-stability}

The ``both keep coins'' strategy (accept inflation, penalize attacker) shows:

\begin{table}[H]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Detection} & \textbf{Penalty} & \textbf{Inflation} & \textbf{Attacker Profit} & \textbf{Stable?} \\
\midrule
50\% & 5x & 1.9\% & $-$\$985 & YES \\
90\% & 5x & 1.1\% & $-$\$1000 & YES \\
99\% & 1x & 4.7\% & $-$\$943 & YES \\
\bottomrule
\end{tabular}
\caption{Economic stability under double-spend attacks}
\label{tab:economic-stability}
\end{table}

\emph{Finding}: Attackers always lose money because trust penalties outweigh gains. Economy is stable (inflation $<$ 5\%) with 5x penalty multiplier even at 50\% detection.

\subsubsection{Finality Latency}
\label{subsubsec:finality-latency}

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Threshold} & \textbf{Connectivity} & \textbf{Median Latency} & \textbf{Success Rate} \\
\midrule
50\% & 0.3 & 0.14s & 100\% \\
70\% & 0.5 & 0.14s & 100\% \\
90\% & 0.7 & 0.14s & 100\% \\
\bottomrule
\end{tabular}
\caption{Finality latency by confirmation threshold}
\label{tab:finality-latency}
\end{table}

\emph{Finding}: Sub-200ms finality achievable. Higher thresholds don't proportionally increase latency because confirmations arrive in parallel through gossip.

\subsubsection{Partition Behavior}

During network partitions, double-spends can temporarily succeed (both victims accept). After healing, all conflicts are detected. This creates a ``damage window'' equal to partition duration.

\emph{Mitigation}: Use ``wait for agreement'' protocol for high-value transactions during suspected partition conditions.

\subsubsection{Currency Weight Spectrum}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Connectivity} & \textbf{Weight} & \textbf{Category} \\
\midrule
0.9 & 0.14 & Lightest (village-level trust) \\
0.5 & 0.34 & Light (town-level) \\
0.1 & 0.80 & Heaviest (needs blockchain bridge) \\
\bottomrule
\end{tabular}
\caption{Currency weight by network connectivity}
\label{tab:currency-weight}
\end{table}

\emph{Conclusion}: Currency weight is proportional to network performance. Better connectivity enables lighter trust mechanisms---the digital equivalent of physical proximity enabling village-level trust at global scale.
